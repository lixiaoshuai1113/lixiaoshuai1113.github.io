üìÅ **PROFESSIONAL EXPERIENCE**

- ü§ñ **Intime AI | Algorithm Intern | May 2025 - August 2025**
  - **LLM-Powered 3D Asset Generation**:Proposed LangMesh, a novel framework leveraging LLMs to interpret natural language and automatically generate complex, editable 3D assets in Blender.
  - **Multi-LLM Agent System for 3D Modeling**:Engineered a collaborative system where different LLMs perform distinct roles: one LLM creates a scene graph for planning, another generates code, and a Vision-Language Model (VLM) autonomously verifies the output for iterative refinement.
  - **Benchmark for Evaluating LLMs in 3D Generation**:Created BlenderBench, the first comprehensive benchmark to systematically assess the 3D code generation and spatial reasoning capabilities of Large Language Models in a Blender environment.


- üöó **Jingwei Hirain | Algorithm Intern | July 2023 - July 2023**
  - **Congestion Prediction Algorithm Reproduction & Optimization**:Independently reproduced a patented congestion prediction algorithm in MATLAB using preceding vehicle speed data. Created a custom dataset by slicing continuous driving videos and iteratively tuned algorithm parameters to enhance detection accuracy.
  - **Novel Algorithm Design**:Designed and implemented an innovative congestion prediction algorithm in MATLAB, which utilizes the Time Headway (THW) of vehicles in adjacent lanes as a key predictive indicator, providing a new approach to traffic state estimation.

üîß **PROJECTS**

- üß† **Vision-Language Model (VLM) Replication and Engineering | Apr 2025 - Mar 2025**
  - Led the end-to-end replication of the R1-V project, successfully executing Supervised Fine-Tuning (SFT) and GRPO reinforcement learning on the Qwen2.5-VL-7B model.
  - Engineered a multi-GPU, multi-node distributed training environment with DeepSpeed, achieving efficient resource scheduling and significantly improving training throughput.
  - Systematically tuned key hyperparameters, including learning rate and batch size, to optimize the trade-off between training speed, model accuracy, and GPU memory consumption.


- üß¨ **LLM Fine-tuning and Mechanism Analysis | Jan 2025 - Feb 2025**
  - Executed both LoRA and full-parameter fine-tuning on leading LLMs (Llama2-7B, Llama3.1-8B) across a diverse range of datasets spanning reasoning, sentiment analysis, mathematics, and code generation.
  - Conducted an in-depth analysis of LoRA by systematically adjusting its core parameters (e.g., rank, target modules) to quantitatively evaluate their impact on model performance and optimize fine-tuning efficiency.
  - Investigated the internal mechanisms of fine-tuned models by analyzing layer-wise activation differences, successfully identifying specific layers critical to performance gains and revealing the correlation between parameter adjustments and model capabilities.
